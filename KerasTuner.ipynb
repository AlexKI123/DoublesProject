{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1G41xFrVdlzj-JQxhxZjmOJQ7Cl2JzrKq",
      "authorship_tag": "ABX9TyNo/k/AU+wB0B7eYtlHZ15L",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlexKI123/DoublesProject/blob/main/KerasTuner.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras-tuner"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sj9pCd7lmb3A",
        "outputId": "585a336a-c37c-465a-f148-cbe9ff4820a9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting keras-tuner\n",
            "  Downloading keras_tuner-1.4.6-py3-none-any.whl (128 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/128.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.9/128.9 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (2.15.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (23.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (2.31.0)\n",
            "Collecting kt-legacy (from keras-tuner)\n",
            "  Downloading kt_legacy-1.0.5-py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (2024.2.2)\n",
            "Installing collected packages: kt-legacy, keras-tuner\n",
            "Successfully installed keras-tuner-1.4.6 kt-legacy-1.0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nYl49Y1RcLi5",
        "outputId": "4c322f3b-c511-45d2-bcac-37677fb11b0a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 30 Complete [01h 05m 12s]\n",
            "val_accuracy: 0.4776539206504822\n",
            "\n",
            "Best val_accuracy So Far: 0.4837740659713745\n",
            "Total elapsed time: 12h 30m 50s\n",
            "\n",
            "The optimal number of units in the first LSTM layer is 100,\n",
            "the optimal number of units in the second LSTM layer is 50,\n",
            "the optimal dropout rate is 0.1, and\n",
            "the optimal number of neurons in the first dense layer is 96.\n",
            "\n",
            "Epoch 1/20\n",
            "21346/21346 [==============================] - 202s 9ms/step - loss: 1.0219 - accuracy: 0.4449 - val_loss: 1.0063 - val_accuracy: 0.4636 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "21346/21346 [==============================] - 197s 9ms/step - loss: 1.0025 - accuracy: 0.4690 - val_loss: 1.0016 - val_accuracy: 0.4701 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "21346/21346 [==============================] - 198s 9ms/step - loss: 1.0011 - accuracy: 0.4711 - val_loss: 1.0002 - val_accuracy: 0.4719 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "21346/21346 [==============================] - 197s 9ms/step - loss: 0.9994 - accuracy: 0.4740 - val_loss: 0.9998 - val_accuracy: 0.4742 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "21346/21346 [==============================] - 197s 9ms/step - loss: 0.9978 - accuracy: 0.4756 - val_loss: 0.9978 - val_accuracy: 0.4757 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "21346/21346 [==============================] - 197s 9ms/step - loss: 0.9962 - accuracy: 0.4774 - val_loss: 0.9971 - val_accuracy: 0.4771 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "21346/21346 [==============================] - 196s 9ms/step - loss: 0.9946 - accuracy: 0.4785 - val_loss: 0.9966 - val_accuracy: 0.4772 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "21346/21346 [==============================] - 193s 9ms/step - loss: 0.9929 - accuracy: 0.4799 - val_loss: 0.9939 - val_accuracy: 0.4784 - lr: 0.0010\n",
            "Epoch 9/20\n",
            "21346/21346 [==============================] - 195s 9ms/step - loss: 0.9913 - accuracy: 0.4821 - val_loss: 0.9949 - val_accuracy: 0.4798 - lr: 0.0010\n",
            "Epoch 10/20\n",
            "21346/21346 [==============================] - 196s 9ms/step - loss: 0.9899 - accuracy: 0.4828 - val_loss: 0.9935 - val_accuracy: 0.4810 - lr: 0.0010\n",
            "Epoch 11/20\n",
            "21346/21346 [==============================] - 195s 9ms/step - loss: 0.9885 - accuracy: 0.4842 - val_loss: 0.9924 - val_accuracy: 0.4814 - lr: 0.0010\n",
            "Epoch 12/20\n",
            "21346/21346 [==============================] - 196s 9ms/step - loss: 0.9870 - accuracy: 0.4852 - val_loss: 0.9919 - val_accuracy: 0.4806 - lr: 0.0010\n",
            "Epoch 13/20\n",
            "21346/21346 [==============================] - 196s 9ms/step - loss: 0.9857 - accuracy: 0.4866 - val_loss: 0.9907 - val_accuracy: 0.4837 - lr: 0.0010\n",
            "Epoch 14/20\n",
            "21346/21346 [==============================] - 196s 9ms/step - loss: 0.9844 - accuracy: 0.4881 - val_loss: 0.9907 - val_accuracy: 0.4807 - lr: 0.0010\n",
            "Epoch 15/20\n",
            "21346/21346 [==============================] - 199s 9ms/step - loss: 0.9832 - accuracy: 0.4896 - val_loss: 0.9903 - val_accuracy: 0.4823 - lr: 0.0010\n",
            "Epoch 16/20\n",
            "21346/21346 [==============================] - 199s 9ms/step - loss: 0.9819 - accuracy: 0.4904 - val_loss: 0.9913 - val_accuracy: 0.4821 - lr: 0.0010\n",
            "Epoch 17/20\n",
            "21346/21346 [==============================] - 196s 9ms/step - loss: 0.9807 - accuracy: 0.4915 - val_loss: 0.9909 - val_accuracy: 0.4819 - lr: 0.0010\n",
            "Epoch 18/20\n",
            "21346/21346 [==============================] - 195s 9ms/step - loss: 0.9794 - accuracy: 0.4922 - val_loss: 0.9908 - val_accuracy: 0.4829 - lr: 0.0010\n",
            "Epoch 19/20\n",
            "21346/21346 [==============================] - 194s 9ms/step - loss: 0.9781 - accuracy: 0.4938 - val_loss: 0.9900 - val_accuracy: 0.4838 - lr: 0.0010\n",
            "Epoch 20/20\n",
            "21346/21346 [==============================] - 195s 9ms/step - loss: 0.9768 - accuracy: 0.4947 - val_loss: 0.9912 - val_accuracy: 0.4816 - lr: 0.0010\n",
            "Test loss: 0.9914873242378235, Test accuracy: 0.4829360842704773\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dropout, Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "import keras_tuner as kt\n",
        "\n",
        "# Load the data\n",
        "train_data = pd.read_csv('/content/drive/MyDrive/input_training.csv', index_col=0)\n",
        "train_data_y = pd.read_csv('/content/drive/MyDrive/output_training_gmEd6Zt.csv', index_col=0)\n",
        "\n",
        "# Merge the input and output data on the 'ID' column\n",
        "merged_data = train_data.merge(train_data_y, left_index=True, right_index=True)\n",
        "\n",
        "# Scale the training data\n",
        "division_value = 100\n",
        "columns_to_transform = [f'r{i}' for i in range(53)]\n",
        "for column in columns_to_transform:\n",
        "    merged_data[column] = merged_data[column] / division_value\n",
        "merged_data[columns_to_transform] = np.tanh(merged_data[columns_to_transform])\n",
        "\n",
        "# Replace NaN values with the mask value\n",
        "mask_value = -2.0\n",
        "merged_data.fillna(mask_value, inplace=True)\n",
        "\n",
        "# Drop the 'day' and 'equity' columns from the DataFrame\n",
        "merged_data = merged_data.drop(['day', 'equity'], axis=1)\n",
        "\n",
        "# Map labels from [-1, 0, 1] to [0, 1, 2]\n",
        "label_mapping = {-1: 0, 0: 1, 1: 2}\n",
        "merged_data['reod'] = merged_data['reod'].map(label_mapping)\n",
        "\n",
        "# Prepare the data\n",
        "X = merged_data.drop('reod', axis=1).values.reshape(-1, 53, 1)\n",
        "y = to_categorical(merged_data['reod'].values)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
        "\n",
        "def build_model(hp):\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(units=hp.Int('units_lstm_1', min_value=60, max_value=140, step=20), return_sequences=True, input_shape=(53, 1)))\n",
        "    model.add(LSTM(units=hp.Int('units_lstm_2', min_value=30, max_value=80, step=20), return_sequences=False))\n",
        "    model.add(Dropout(rate=hp.Float('dropout', min_value=0.0, max_value=0.5, step=0.05)))\n",
        "    model.add(Dense(units=hp.Int('dense_units', min_value=32, max_value=96, step=32), activation='relu'))\n",
        "    model.add(Dense(3, activation='softmax'))\n",
        "    model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "tuner = kt.Hyperband(build_model, objective='val_accuracy', max_epochs=20, directory='keras_tuner_dir', project_name='lstm_optimization')\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, min_lr=0.00001)\n",
        "#early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "\n",
        "# Execute the search\n",
        "tuner.search(X_train, y_train, epochs=20, validation_split=0.2, callbacks=[reduce_lr])\n",
        "\n",
        "# Get the optimal hyperparameters\n",
        "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "\n",
        "print(f\"\"\"\n",
        "The optimal number of units in the first LSTM layer is {best_hps.get('units_lstm_1')},\n",
        "the optimal number of units in the second LSTM layer is {best_hps.get('units_lstm_2')},\n",
        "the optimal dropout rate is {best_hps.get('dropout')}, and\n",
        "the optimal number of neurons in the first dense layer is {best_hps.get('dense_units')}.\n",
        "\"\"\")\n",
        "\n",
        "# Build the model with the optimal hyperparameters and train it on the data\n",
        "model = tuner.hypermodel.build(best_hps)\n",
        "history = model.fit(X_train, y_train, epochs=20, validation_split=0.1, callbacks=[reduce_lr])\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(f'Test loss: {loss}, Test accuracy: {accuracy}')\n"
      ]
    }
  ]
}